{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from prawcore.exceptions import RequestException, ResponseException, OAuthException\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id='aC3FI1WNM2F5uaSSJF2rEw', \n",
    "                     client_secret='y7BQGM0bRxCVyPEh4oQnioS2rW57ZQ', \n",
    "                     user_agent='gryphonholder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('ADHD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "posts = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction count: 1000\n",
      "Extraction rate (last 1000): 429.83 extractions/second\n",
      "Extraction time (last 1000): 2.33 seconds taken\n",
      "Title of the post: What is the weirdest way you trick your ADHD brain?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m \u001b[39m300000\u001b[39m:  \u001b[39m# Adjust the condition for the desired total count\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         \u001b[39mfor\u001b[39;00m submission \u001b[39min\u001b[39;00m subreddit\u001b[39m.\u001b[39mnew(limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m             \u001b[39mif\u001b[39;00m submission\u001b[39m.\u001b[39mid \u001b[39min\u001b[39;00m processed_ids:\n\u001b[1;32m     22\u001b[0m                 \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Skip this post if it has been processed before\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/models/listing/generator.py:63\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_batch()\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/models/listing/generator.py:89\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exhausted:\n\u001b[1;32m     87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reddit\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_sublist(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39m@_deprecate_args\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\n\u001b[1;32m    701\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    704\u001b[0m     params: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    705\u001b[0m ):\n\u001b[1;32m    706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \n\u001b[1;32m    708\u001b[0m \u001b[39m    :param path: The path to fetch.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \n\u001b[1;32m    711\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objectify_request(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objectify_request\u001b[39m(\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    493\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m     path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    500\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    501\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[39m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objector\u001b[39m.\u001b[39mobjectify(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    518\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    519\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    520\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    521\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    522\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    523\u001b[0m             path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/praw/reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[1;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/prawcore/sessions.py:330\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[1;32m    331\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    332\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    333\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    334\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    335\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    336\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    337\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/prawcore/sessions.py:228\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    226\u001b[0m retry_strategy_state\u001b[39m.\u001b[39msleep()\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(data, method, params, url)\n\u001b[0;32m--> 228\u001b[0m response, saved_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    229\u001b[0m     data,\n\u001b[1;32m    230\u001b[0m     files,\n\u001b[1;32m    231\u001b[0m     json,\n\u001b[1;32m    232\u001b[0m     method,\n\u001b[1;32m    233\u001b[0m     params,\n\u001b[1;32m    234\u001b[0m     retry_strategy_state,\n\u001b[1;32m    235\u001b[0m     timeout,\n\u001b[1;32m    236\u001b[0m     url,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    239\u001b[0m do_retry \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    241\u001b[0m     response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39munauthorized\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/prawcore/sessions.py:185\u001b[0m, in \u001b[0;36mSession._make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    175\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     url,\n\u001b[1;32m    183\u001b[0m ):\n\u001b[1;32m    184\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rate_limiter\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m    186\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requestor\u001b[39m.\u001b[39;49mrequest,\n\u001b[1;32m    187\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_header_callback,\n\u001b[1;32m    188\u001b[0m             method,\n\u001b[1;32m    189\u001b[0m             url,\n\u001b[1;32m    190\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    191\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    192\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    193\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    194\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    195\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    198\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m bytes)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/prawcore/rate_limit.py:32\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, request_function, set_header_callback, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Rate limit the call to request_function.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m    :param request_function: A function call that returns an HTTP response object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdelay()\n\u001b[1;32m     33\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m set_header_callback()\n\u001b[1;32m     34\u001b[0m     response \u001b[39m=\u001b[39m request_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/prawcore/rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.delay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSleeping: \u001b[39m\u001b[39m{\u001b[39;00msleep_seconds\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds prior to call\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m log\u001b[39m.\u001b[39mdebug(message)\n\u001b[0;32m---> 47\u001b[0m time\u001b[39m.\u001b[39;49msleep(sleep_seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('ADHD')\n",
    "count = 0\n",
    "posts = []\n",
    "processed_ids = set()  # This will store the IDs of the processed posts\n",
    "\n",
    "# Check if a count file exists and load the count from there\n",
    "if os.path.exists('count.txt'):\n",
    "    with open('count.txt', 'r') as f:\n",
    "        count = int(f.read())\n",
    "\n",
    "# Check if a posts file exists and load the posts from there\n",
    "if os.path.exists('ADHDposts.csv'):\n",
    "    df = pd.read_csv('ADHDposts.csv')\n",
    "    posts = df.to_dict('records')\n",
    "\n",
    "start_time = time.time()  # Initialize start_time\n",
    "\n",
    "while count < 300000:  # Adjust the condition for the desired total count\n",
    "    try:\n",
    "        for submission in subreddit.new(limit=None):\n",
    "            if submission.id in processed_ids:\n",
    "                continue  # Skip this post if it has been processed before\n",
    "            posts.append({\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"text\": submission.selftext,\n",
    "                \"date\": submission.created_utc,\n",
    "                \"upvotes\": submission.score,\n",
    "            })\n",
    "            count += 1\n",
    "            processed_ids.add(submission.id)  # Add the post ID to the set of processed IDs\n",
    "            if count % 1000 == 0:\n",
    "                end_time = time.time()  # End time after 1000 extractions\n",
    "                print(f\"Extraction count: {count}\")\n",
    "                rate = 1000 / (end_time - start_time)\n",
    "                print(f\"Extraction rate (last 1000): {rate:.2f} extractions/second\")\n",
    "                print(f\"Extraction time (last 1000): {1000/rate:.2f} seconds taken\")\n",
    "                print(f\"Title of the post: {submission.title}\")\n",
    "                start_time = time.time()  # Reset start_time for next 1000 extractions\n",
    "    except (RequestException, OAuthException, ResponseException) as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Sleeping for 60 seconds before trying again.\")\n",
    "        time.sleep(60)\n",
    "    finally:\n",
    "        # Save the count to a file\n",
    "        with open('count.txt', 'w') as f:\n",
    "            f.write(str(count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subreddit = reddit.subreddit(\"ADHD\")  # replace with your subreddit\n",
    "\n",
    "posts = []\n",
    "start_time = time.time()\n",
    "\n",
    "for count, submission in enumerate(subreddit.new(limit=10000), 1):\n",
    "    posts.append({\n",
    "        \"id\": submission.id,\n",
    "        \"title\": submission.title,\n",
    "        \"text\": submission.selftext,\n",
    "        \"score\": submission.score\n",
    "    })\n",
    "\n",
    "    if count % 500 == 0:\n",
    "        end_time = time.time()\n",
    "        rate = 500 / (end_time - start_time)\n",
    "        print(f\"Extraction count: {count}\")\n",
    "        print(f\"Extraction rate (last 500): {rate:.2f} extractions/second\")\n",
    "        print(f\"Extraction time (last 500): {500/rate:.2f} seconds taken\")\n",
    "        print(f\"Title of the last post: {submission.title}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "df = pd.DataFrame(posts)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped from the previous interval: 0\n",
      "Number of posts scraped from the previous interval: 0\n",
      "Number of posts scraped from the previous interval: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m current_date \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m timedelta(days\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Sleep for 30 seconds to avoid hitting Reddit's rate limit\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m30\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the subreddit\n",
    "subreddit = reddit.subreddit('ADHD')\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2023, 6, 1)\n",
    "end_date = datetime(2020, 6, 23)  # Reddit's launch date\n",
    "\n",
    "# Initialize an empty DataFrame to store the posts\n",
    "posts_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a counter for the number of posts\n",
    "post_count = 0\n",
    "\n",
    "# Loop over each 3-day interval in the date range\n",
    "current_date = start_date\n",
    "while current_date > end_date:\n",
    "    # Define the query string\n",
    "    query = f'timestamp:{int((current_date - timedelta(days=3)).timestamp())}..{int(current_date.timestamp())}'\n",
    "\n",
    "    # Fetch the posts\n",
    "    posts = subreddit.search(query, sort='new', limit=None, time_filter='all')\n",
    "\n",
    "    # Initialize a counter for the number of posts in this interval\n",
    "    interval_post_count = 0\n",
    "\n",
    "    # Initialize a variable to store the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the posts and append them to the DataFrame\n",
    "    for post in posts:\n",
    "        posts_df = posts_df.append({\n",
    "            'title': post.title,\n",
    "            'score': post.score,\n",
    "            'id': post.id,\n",
    "            'body': post.selftext\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Increment the counters\n",
    "        post_count += 1\n",
    "        interval_post_count += 1\n",
    "\n",
    "        # Print the title of the first post of the interval\n",
    "        if interval_post_count == 1:\n",
    "            print(f\"Scraping posts from {current_date - timedelta(days=3)} to {current_date}\")\n",
    "            print(f\"Title of the first post: {post.title}\")\n",
    "\n",
    "        # Print the rate of scraping every 10 seconds\n",
    "        if time.time() - start_time >= 10:\n",
    "            print(f\"Rate of scraping: {interval_post_count / (time.time() - start_time):.2f} posts/second\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Print the number of posts scraped from the previous interval\n",
    "    print(f\"Number of posts scraped from the previous interval: {interval_post_count}\")\n",
    "\n",
    "    # Decrement the current date\n",
    "    current_date -= timedelta(days=3)\n",
    "\n",
    "    # Sleep for 30 seconds to avoid hitting Reddit's rate limit\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4 - using pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     32\u001b[0m \u001b[39m# Loop over the posts and append them to the DataFrame\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[1;32m     34\u001b[0m     posts_df \u001b[39m=\u001b[39m posts_df\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     35\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m: post[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m: post[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     37\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: post[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m: post[\u001b[39m'\u001b[39m\u001b[39mselftext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m     }, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Increment the counters\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# Define the subreddit\n",
    "subreddit = 'ADHD'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2023, 6, 1)\n",
    "end_date = datetime(2020, 6, 23)  # Reddit's launch date\n",
    "\n",
    "# Initialize an empty DataFrame to store the posts\n",
    "posts_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a counter for the number of posts\n",
    "post_count = 0\n",
    "\n",
    "# Loop over each 3-day interval in the date range\n",
    "current_date = start_date\n",
    "while current_date > end_date:\n",
    "    # Define the query string\n",
    "    after = int((current_date - timedelta(days=3)).timestamp())\n",
    "    before = int(current_date.timestamp())\n",
    "\n",
    "    # Fetch the posts\n",
    "    url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&after={after}&before={before}&sort=desc\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Initialize a counter for the number of posts in this interval\n",
    "    interval_post_count = 0\n",
    "\n",
    "    # Initialize a variable to store the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the posts and append them to the DataFrame\n",
    "    for post in data['data']:\n",
    "        posts_df = posts_df.append({\n",
    "            'title': post['title'],\n",
    "            'score': post['score'],\n",
    "            'id': post['id'],\n",
    "            'body': post['selftext']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Increment the counters\n",
    "        post_count += 1\n",
    "        interval_post_count += 1\n",
    "\n",
    "        # Print the title of the first post of the interval\n",
    "        if interval_post_count == 1:\n",
    "            print(f\"Scraping posts from {current_date - timedelta(days=3)} to {current_date}\")\n",
    "            print(f\"Title of the first post: {post['title']}\")\n",
    "\n",
    "        # Print the rate of scraping every 10 seconds\n",
    "        if time.time() - start_time >= 10:\n",
    "            print(f\"Rate of scraping: {interval_post_count / (time.time() - start_time):.2f} posts/second\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Print the number of posts scraped from the previous interval\n",
    "    print(f\"Number of posts scraped from the previous interval: {interval_post_count}\")\n",
    "\n",
    "    # Decrement the current date\n",
    "    current_date -= timedelta(days=3)\n",
    "\n",
    "    # Sleep for 30 seconds to avoid hitting Reddit's rate limit\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althea's Version (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "reddit = praw.Reddit(client_id ='zWndV5hUJ8XZYshw1V9axw',\n",
    "                     client_secret ='4ovj54M6J9s4yS6alG7c58jKOa3TQA',\n",
    "                     user_agent ='my user agent')\n",
    "subs = ['bipolar', 'schizophrenia']\n",
    "submissions_types = ['hot', 'controversial', 'new', 'rising', 'top']\n",
    "for x in subs:\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(x)\n",
    "\n",
    "    for submission_type in submissions_types:\n",
    "        submission_generator = getattr(subreddit, submission_type)(limit=1000)\n",
    "        posts.extend([[submission.id, submission.title, submission.selftext, submission.score] for submission in submission_generator])\n",
    "        print(f'{submission_type.capitalize()} posts from {x} subreddit scraped')\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['id', 'title', 'text', 'score'])\n",
    "    df.to_csv(f'{x}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the posts to a CSV file\n",
    "df = pd.DataFrame(posts)\n",
    "df.to_csv('ADHD_10K_notcleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated('title', keep=False)]\n",
    "duplicate_counts = duplicates['title'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "print(duplicate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
