{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This ipynb is for cleaning a preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import combined.csv dataset, but drop index\n",
    "df = pd.read_csv('../data/combined.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'text', 'score', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Print columns of df\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing other nltk libraries\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "\n",
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # We'll talk about this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Tokenize text\n",
    "        words = tokenizer.tokenize(text)\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        # Lemmatize words\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        # Join words back into a string\n",
    "        text = ' '.join(words)\n",
    "    else:\n",
    "        # If the input is not a string, return an empty string\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv as combined_cleaned.csv\n",
    "df.to_csv('../data/combined_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "0    many people experience increased mania summer ...\n",
      "2    noticed today kept randomly laughing stuff isn...\n",
      "3    manic two week yesterday fell isnt surprising ...\n",
      "4    school teacher love class couple extreme behav...\n",
      "5    anyone feel like lone forever try date feeling...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514\n"
     ]
    }
   ],
   "source": [
    "#Find number of rows in df\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
